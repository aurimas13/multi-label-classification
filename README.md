# Multi-Label Image Classification with Custom CNN

A custom neural network implementation for multi-label image classification built from scratch using PyTorch.

## ğŸ¯ Project Overview

This project implements a custom Convolutional Neural Network (CNN) for multi-label classification where each image can belong to multiple classes simultaneously. The implementation includes:

- **Custom CNN Architecture**: 4-layer convolutional network with batch normalization and dropout
- **Complete Training Loop**: Manual implementation without high-level `.fit()` methods
- **Multi-Label Support**: Uses Binary Cross-Entropy loss for independent label predictions
- **Comprehensive Metrics**: Hamming loss, exact match accuracy, precision, recall, and F1-score
- **Inference Pipeline**: Test evaluation and single-sample prediction demonstration

## ğŸ—ï¸ Architecture Details

### Model Architecture
```
Input (224x224x3)
    â†“
Conv Block 1: Conv(3â†’32) + BatchNorm + ReLU + MaxPool
    â†“
Conv Block 2: Conv(32â†’64) + BatchNorm + ReLU + MaxPool
    â†“
Conv Block 3: Conv(64â†’128) + BatchNorm + ReLU + MaxPool
    â†“
Conv Block 4: Conv(128â†’256) + BatchNorm + ReLU
    â†“
Global Average Pooling
    â†“
FC Layer 1: Linear(256â†’512) + ReLU + Dropout(0.5)
    â†“
FC Layer 2: Linear(512â†’num_classes)
    â†“
Output (num_classes)
```

### Key Design Decisions

1. **BCEWithLogitsLoss**: Combines sigmoid activation with BCE loss for numerical stability
2. **Global Average Pooling**: Reduces parameters and improves generalization vs traditional flatten
3. **Batch Normalization**: Stabilizes training and accelerates convergence
4. **Dropout (0.5)**: Prevents overfitting by randomly deactivating neurons during training
5. **Adam Optimizer**: Adaptive learning rates with momentum for efficient optimization

## ğŸ“Š Metrics Explained

- **Hamming Loss**: Fraction of wrong labels (lower is better)
- **Exact Match**: Percentage of samples with ALL labels correctly predicted
- **Precision**: Of predicted positive labels, how many were correct?
- **Recall**: Of actual positive labels, how many were found?
- **F1-Score**: Harmonic mean of precision and recall

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install torch torchvision numpy matplotlib scikit-learn pillow
```

### Running the Code
```bash
python multilabel_classification.py
```

### Expected Output
- Training progress with loss per epoch
- Validation metrics after each epoch
- Final test set evaluation
- Sample inference with predicted labels
- Training history plots saved as `training_history.png`
- Trained model saved as `multilabel_model.pth`

## ğŸ“ Project Structure
```
multi-label-classification/
â”‚
â”œâ”€â”€ multilabel_classification.py  # Main implementation
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore                     # Git ignore file
â”‚
â”œâ”€â”€ training_history.png           # Generated after training
â””â”€â”€ multilabel_model.pth           # Saved model checkpoint
```

## ğŸ’¡ Usage with Real Data

To use with real image data (e.g., Open Images dataset):

1. **Download Dataset**:
```python
# Example for Open Images subset
# Download images and annotations
# Parse multi-label annotations
```

2. **Modify Dataset Class**:
```python
def __getitem__(self, idx):
    # Replace synthetic data with:
    img = Image.open(self.image_paths[idx]).convert('RGB')
    # Rest remains the same
```

3. **Update Data Loading**:
```python
# Parse real annotations
# Create train/val/test splits
# Pass real paths and labels to MultiLabelDataset
```

## ğŸ”§ Customization Options

### Hyperparameters
- `NUM_CLASSES`: Number of possible labels (default: 4)
- `BATCH_SIZE`: Samples per gradient update (default: 32)
- `LEARNING_RATE`: Optimizer step size (default: 0.001)
- `NUM_EPOCHS`: Training iterations (default: 10)

### Model Architecture
- Adjust convolutional layers depths
- Modify dropout rates
- Change pooling strategies
- Add/remove layers

### Data Augmentation
- Adjust augmentation probabilities
- Add new transformations (crop, blur, etc.)
- Modify normalization statistics

## ğŸ“ˆ Performance Considerations

1. **GPU Usage**: Automatically uses CUDA if available
2. **Memory Efficiency**: Lazy loading of images
3. **Batch Processing**: Efficient mini-batch gradient descent
4. **Mixed Precision**: Can add `torch.cuda.amp` for faster training

## ğŸ§ª Testing

The code includes comprehensive testing:
- Validation after each epoch
- Final test set evaluation
- Single sample inference demonstration
- Metric calculation and visualization

## ğŸ“ Notes

- The current implementation uses **synthetic data** for demonstration
- For production use, replace with actual image dataset
- Model performance depends on data quality and quantity
- Consider transfer learning for better performance on small datasets

## ğŸ¤ Contributing

Feel free to submit issues and enhancement requests!

## ğŸ“„ License

MIT License - feel free to use in your projects

## ğŸ™ Acknowledgments

- PyTorch team for the excellent deep learning framework
- Multi-label classification research community
- Open Images dataset creators (when using real data)