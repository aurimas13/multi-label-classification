# -*- coding: utf-8 -*-
"""Multi-Label Image Classification - Google Colab Optimized Version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1re3-YLSqYBNjeibqdekUPukyIEju429r
"""

### CELL 1: Setup and Imports ###

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from torch.cuda.amp import autocast, GradScaler
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support, hamming_loss, accuracy_score
import os
import json
from typing import Tuple, List, Dict
import random
import warnings
from tqdm import tqdm
import time
warnings.filterwarnings('ignore')

# Check if running on Colab
try:
    import google.colab
    IN_COLAB = True
    print("🎉 Running on Google Colab!")
    # Mount Google Drive if needed
    # from google.colab import drive
    # drive.mount('/content/drive')
except:
    IN_COLAB = False
    print("Running locally")

# Set random seeds for reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# Device configuration - prioritize CUDA
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"🚀 Using GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
elif torch.backends.mps.is_available():
    device = torch.device('mps')
    print("🍎 Using Apple Metal Performance Shaders")
else:
    device = torch.device('cpu')
    print("💻 Using CPU (Training will be slower)")

"""### CELL 2: Understanding Metrics - DETAILED EXPLANATION ###

📊 METRICS EXPLAINED:

1. **LOSS (0.6931)**:
   - Binary Cross-Entropy loss for multi-label classification
   - Starting at 0.693 is EXPECTED (it's -ln(0.5) = random guessing)
   - Good model should reach 0.1-0.3
   - My first running of the model gave loss that didn't decrease much = model didn't learn properly

2. **HAMMING LOSS (0.4833)**:
   - Fraction of wrong labels per sample
   - Example: If true=[1,0,1,0] and pred=[1,1,1,0], hamming=0.25 (1 wrong out of 4)
   - 0.4833 means ~48% of individual labels are wrong (BAD - random is 50%)

3. **EXACT MATCH (0.1267)**:
   - % of samples where ALL labels are correct
   - Very strict metric
   - My first running of the model gave 12.67% which is poor but expected with random data

4. **PRECISION (0.4800)**:
   - Of all positive predictions, how many were correct?
   - "When model says 'yes', how often is it right?"

5. **RECALL (0.2700)**:
   - Of all actual positives, how many were found?
   - "Of all true labels, how many did model find?"
   - My first running of the model gave 27% recall that is LOW - model missed most positive labels

6. **F1 SCORE (0.3289)**:
   - Harmonic mean of Precision and Recall
   - Balanced metric between the two
   - Good models achieve 0.7-0.9

7. **EPOCH**:
   - One complete pass through entire training dataset
   - More epochs = more learning opportunities
   - Need 30-100 epochs for good performance

8. **BATCH**:
   - Number of samples processed before updating weights
   - Batch 32 = process 32 images, then update
   - Larger batch = more stable but needs more memory

WHY MY LOSS WAS BAD IN THE FIRST RUN:
- Used synthetic random data (no patterns to learn!)
- Only 10 epochs (too little)
- Model might had been too simple
- Learning rate might had needed tuning
"""

### CELL 3: Optimized Dataset with Real Patterns ###

class OptimizedMultiLabelDataset(Dataset):
    """
    Optimized dataset that creates learnable patterns instead of pure random noise
    """

    def __init__(self, num_samples: int = 1000, num_classes: int = 4,
                 mode: str = 'train', transform=None):
        self.num_samples = num_samples
        self.num_classes = num_classes
        self.mode = mode
        self.transform = transform

        # Create synthetic but LEARNABLE patterns
        # This creates images with actual patterns the model can learn
        np.random.seed(42 if mode == 'train' else 43 if mode == 'val' else 44)

        self.data = []
        self.labels = []

        for i in range(num_samples):
            # Create image with patterns based on class
            img = np.zeros((224, 224, 3), dtype=np.uint8)
            label = np.zeros(num_classes)

            # Add patterns for each class (so model can actually learn something!)
            # Class 0: Horizontal stripes
            if np.random.random() > 0.5:
                for y in range(0, 224, 20):
                    img[y:y+10, :, 0] = 255
                label[0] = 1

            # Class 1: Vertical stripes
            if np.random.random() > 0.5:
                for x in range(0, 224, 20):
                    img[:, x:x+10, 1] = 255
                label[1] = 1

            # Class 2: Diagonal pattern
            if np.random.random() > 0.5:
                for i in range(224):
                    img[i, i, 2] = 255
                    if i > 0:
                        img[i-1, i, 2] = 200
                    if i < 223:
                        img[i+1, i, 2] = 200
                label[2] = 1

            # Class 3: Circle pattern
            if np.random.random() > 0.5:
                center = (112, 112)
                radius = 50
                y, x = np.ogrid[:224, :224]
                mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
                img[mask] = [255, 255, 0]
                label[3] = 1

            # Ensure at least one label is active
            if label.sum() == 0:
                label[np.random.randint(0, num_classes)] = 1

            # Add some noise for regularization
            noise = np.random.randn(224, 224, 3) * 20
            img = np.clip(img.astype(np.float32) + noise, 0, 255).astype(np.uint8)

            self.data.append(img)
            self.labels.append(label)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        img = Image.fromarray(self.data[idx])

        if self.transform:
            img = self.transform(img)

        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return img, label

### CELL 4: Enhanced CNN Architecture ###

class EnhancedCNN(nn.Module):
    """
    Improved CNN with better architecture for learning
    """

    def __init__(self, num_classes: int = 4, dropout_rate: float = 0.3):
        super(EnhancedCNN, self).__init__()

        # Deeper architecture with residual connections
        # Initial convolution
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Residual blocks for better gradient flow
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)

        # Global average pooling
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        # Classification head with dropout
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, num_classes)

        # Initialize weights
        self._initialize_weights()

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)

        x = self.dropout(x)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

class ResidualBlock(nn.Module):
    """Residual block for better gradient flow"""

    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = self.shortcut(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += identity
        out = self.relu(out)

        return out

### CELL 5: Advanced Training Functions ###

def train_epoch_optimized(model, dataloader, criterion, optimizer, scaler, epoch, total_epochs):
    """Optimized training with mixed precision and progress bar"""
    model.train()
    total_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    # Progress bar
    pbar = tqdm(dataloader, desc=f'Epoch {epoch}/{total_epochs}')

    for batch_idx, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Mixed precision training for faster computation
        if device.type == 'cuda':
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        total_loss += loss.item()

        # Calculate accuracy for progress bar
        predictions = (torch.sigmoid(outputs) > 0.5).float()
        correct_predictions += (predictions == labels).sum().item()
        total_predictions += labels.numel()

        # Update progress bar
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
            'acc': f'{correct_predictions/total_predictions:.2%}'
        })

    return total_loss / len(dataloader)

def validate_epoch_optimized(model, dataloader, criterion):
    """Optimized validation"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc='Validating'):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            all_predictions.append(outputs)
            all_targets.append(labels)

    all_predictions = torch.cat(all_predictions)
    all_targets = torch.cat(all_targets)

    metrics = calculate_metrics(all_predictions, all_targets)
    avg_loss = total_loss / len(dataloader)

    return avg_loss, metrics

def calculate_metrics(predictions, targets, threshold=0.5):
    """Calculate comprehensive metrics"""
    preds = (torch.sigmoid(predictions) > threshold).cpu().numpy()
    targets = targets.cpu().numpy()

    h_loss = hamming_loss(targets, preds)
    exact_match = accuracy_score(targets, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        targets, preds, average='samples', zero_division=0
    )

    return {
        'hamming_loss': h_loss,
        'exact_match': exact_match,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }

def train_model(config=None):
    """
    Main training function with optimizations
    """
    # Default configuration
    if config is None:
        config = {
            'num_classes': 4,
            'batch_size': 64 if device.type == 'cuda' else 32,
            'learning_rate': 0.001,
            'num_epochs': 50,  # Increased for better learning
            'weight_decay': 1e-4,  # L2 regularization
            'scheduler': 'cosine',  # Learning rate scheduling
            'dropout': 0.3,
            'num_samples': 2000  # More data
        }

    print("🚀 Optimized Training Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")

    # Data augmentation - more aggressive for better generalization
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.3),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create datasets with learnable patterns
    print("\n📊 Creating datasets with learnable patterns...")
    train_dataset = OptimizedMultiLabelDataset(
        num_samples=config['num_samples'],
        num_classes=config['num_classes'],
        mode='train',
        transform=train_transform
    )
    val_dataset = OptimizedMultiLabelDataset(
        num_samples=config['num_samples']//5,
        num_classes=config['num_classes'],
        mode='val',
        transform=val_transform
    )
    test_dataset = OptimizedMultiLabelDataset(
        num_samples=config['num_samples']//5,
        num_classes=config['num_classes'],
        mode='test',
        transform=val_transform
    )

    # DataLoaders with optimizations
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=2 if device.type == 'cuda' else 0,
        pin_memory=True if device.type == 'cuda' else False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=2 if device.type == 'cuda' else 0,
        pin_memory=True if device.type == 'cuda' else False
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False
    )

    print(f"✅ Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

    # Initialize model
    model = EnhancedCNN(
        num_classes=config['num_classes'],
        dropout_rate=config['dropout']
    ).to(device)

    print(f"🧠 Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Loss function with class weights for imbalanced data
    criterion = nn.BCEWithLogitsLoss()

    # Optimizer with weight decay for regularization
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )

    # Learning rate scheduler for better convergence
    if config['scheduler'] == 'cosine':
        scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    else:
        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

    # Mixed precision scaler for faster training
    scaler = GradScaler() if device.type == 'cuda' else None

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_f1': [],
        'val_precision': [],
        'val_recall': [],
        'learning_rates': []
    }

    best_f1 = 0
    best_epoch = 0

    print("\n🎯 Starting Optimized Training...")
    print("="*50)

    start_time = time.time()

    for epoch in range(1, config['num_epochs'] + 1):
        # Training
        train_loss = train_epoch_optimized(
            model, train_loader, criterion, optimizer, scaler, epoch, config['num_epochs']
        )

        # Validation
        val_loss, val_metrics = validate_epoch_optimized(model, val_loader, criterion)

        # Update learning rate
        if config['scheduler'] == 'cosine':
            scheduler.step()
        else:
            scheduler.step(val_loss)

        # Store history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_f1'].append(val_metrics['f1_score'])
        history['val_precision'].append(val_metrics['precision'])
        history['val_recall'].append(val_metrics['recall'])
        history['learning_rates'].append(optimizer.param_groups[0]['lr'])

        # Print epoch summary
        print(f"\n📈 Epoch {epoch}/{config['num_epochs']} Summary:")
        print(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        print(f"  Val F1: {val_metrics['f1_score']:.4f} | Precision: {val_metrics['precision']:.4f} | Recall: {val_metrics['recall']:.4f}")
        print(f"  Hamming Loss: {val_metrics['hamming_loss']:.4f} | Exact Match: {val_metrics['exact_match']:.4f}")
        print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")

        # Save best model
        if val_metrics['f1_score'] > best_f1:
            best_f1 = val_metrics['f1_score']
            best_epoch = epoch
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_f1': best_f1,
                'config': config
            }, 'best_model.pth')
            print(f"  💾 New best model saved! F1: {best_f1:.4f}")

        print("-"*50)

    training_time = time.time() - start_time
    print(f"\n✅ Training completed in {training_time:.2f} seconds")
    print(f"🏆 Best F1 Score: {best_f1:.4f} at epoch {best_epoch}")

    # Load best model for final evaluation
    checkpoint = torch.load('best_model.pth', map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])

    # Final test evaluation
    print("\n🎯 Final Test Evaluation (Best Model):")
    test_loss, test_metrics = validate_epoch_optimized(model, test_loader, criterion)
    print(f"  Test Loss: {test_loss:.4f}")
    print(f"  Test F1: {test_metrics['f1_score']:.4f}")
    print(f"  Test Precision: {test_metrics['precision']:.4f}")
    print(f"  Test Recall: {test_metrics['recall']:.4f}")
    print(f"  Test Hamming Loss: {test_metrics['hamming_loss']:.4f}")
    print(f"  Test Exact Match: {test_metrics['exact_match']:.4f}")

    # Plot results
    plot_training_history(history)

    return model, history, test_metrics, test_loader  # Added test_loader to return

### CELL 7: Visualization Functions ###

def plot_training_history(history):
    """Create comprehensive training plots"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    fig.suptitle('Training History', fontsize=16)

    # Loss plot
    axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)
    axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss Curves')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # F1 Score plot
    axes[0, 1].plot(history['val_f1'], label='Val F1', color='green', linewidth=2)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('F1 Score')
    axes[0, 1].set_title('F1 Score Progress')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Precision/Recall plot
    axes[0, 2].plot(history['val_precision'], label='Precision', color='blue', linewidth=2)
    axes[0, 2].plot(history['val_recall'], label='Recall', color='red', linewidth=2)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Score')
    axes[0, 2].set_title('Precision vs Recall')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Learning rate plot
    axes[1, 0].plot(history['learning_rates'], color='orange', linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].set_title('Learning Rate Schedule')
    axes[1, 0].grid(True, alpha=0.3)

    # Loss difference plot
    loss_diff = np.array(history['val_loss']) - np.array(history['train_loss'])
    axes[1, 1].plot(loss_diff, color='purple', linewidth=2)
    axes[1, 1].axhline(y=0, color='gray', linestyle='--')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Val Loss - Train Loss')
    axes[1, 1].set_title('Overfitting Monitor')
    axes[1, 1].grid(True, alpha=0.3)

    # F1 improvement rate
    f1_improvement = np.diff(history['val_f1'])
    axes[1, 2].bar(range(1, len(f1_improvement) + 1), f1_improvement, color='teal')
    axes[1, 2].axhline(y=0, color='gray', linestyle='--')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('F1 Change')
    axes[1, 2].set_title('F1 Score Improvement per Epoch')
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('optimized_training_history.png', dpi=100, bbox_inches='tight')
    plt.show()
    print("📊 Training plots saved to 'optimized_training_history.png'")

### CELL 8: Run Everything ###

if __name__ == "__main__":
    # Configuration for better performance
    config = {
        'num_classes': 4,
        'batch_size': 128 if device.type == 'cuda' else 32,  # Larger batch for GPU
        'learning_rate': 0.001,
        'num_epochs': 50,  # More epochs for better learning
        'weight_decay': 1e-4,
        'scheduler': 'cosine',
        'dropout': 0.3,
        'num_samples': 3000  # More training data
    }

    # Train the model
    model, history, test_metrics, test_loader = train_model(config)  # Updated to receive test_loader

    # Print final summary
    print("\n" + "="*50)
    print("🎊 TRAINING COMPLETE - FINAL SUMMARY")
    print("="*50)
    print(f"Final Test F1 Score: {test_metrics['f1_score']:.4f}")
    print(f"Final Test Hamming Loss: {test_metrics['hamming_loss']:.4f}")
    print(f"Final Test Exact Match: {test_metrics['exact_match']:.2%}")
    print("\n✨ Model and results saved successfully!")

def analyze_model_predictions(model, dataloader, class_names=['Class 0', 'Class 1', 'Class 2', 'Class 3']):
    """Analyze model predictions in detail"""
    model.eval()

    # Get a batch for analysis
    images, labels = next(iter(dataloader))
    images = images[:4].to(device)  # Take first 4 samples
    labels = labels[:4]

    with torch.no_grad():
        outputs = model(images)
        probs = torch.sigmoid(outputs).cpu()

    # Create visualization
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    fig.suptitle('Sample Predictions Analysis', fontsize=16)

    for i in range(4):
        # Show image
        img = images[i].cpu()
        # Denormalize
        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        img = torch.clamp(img, 0, 1)

        axes[0, i].imshow(img.permute(1, 2, 0))
        axes[0, i].set_title(f'Sample {i+1}')
        axes[0, i].axis('off')

        # Show predictions
        x = np.arange(len(class_names))
        colors = ['green' if probs[i, j] > 0.5 else 'red' for j in range(len(class_names))]
        bars = axes[1, i].bar(x, probs[i], color=colors, alpha=0.7)
        axes[1, i].set_ylim([0, 1])
        axes[1, i].set_xticks(x)
        axes[1, i].set_xticklabels(class_names, rotation=45)
        axes[1, i].set_ylabel('Probability')
        axes[1, i].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)

        # Add true labels
        true_indices = torch.where(labels[i] == 1)[0]
        title = f"True: {[class_names[idx] for idx in true_indices]}"
        axes[1, i].set_title(title, fontsize=10)

        # Add probability values on bars
        for bar, prob in zip(bars, probs[i]):
            height = bar.get_height()
            axes[1, i].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                          f'{prob:.2f}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig('prediction_analysis.png', dpi=100, bbox_inches='tight')
    plt.show()
    print("🔍 Prediction analysis saved to 'prediction_analysis.png'")

# Create a test dataloader for analysis (if not already created)
# This recreates the test_loader that was created inside train_model()
if 'test_loader' not in globals():
    print("Creating test dataloader for analysis...")
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    test_dataset = OptimizedMultiLabelDataset(
        num_samples=400,  # Small test set
        num_classes=4,
        mode='test',
        transform=test_transform
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=32,
        shuffle=False
    )
    print(f"✅ Test dataloader created with {len(test_dataset)} samples")

# Now run the analysis
if 'model' in globals():
    analyze_model_predictions(model, test_loader)
else:
    print("⚠️ Please run the training cell (Cell 8) first to train the model!")